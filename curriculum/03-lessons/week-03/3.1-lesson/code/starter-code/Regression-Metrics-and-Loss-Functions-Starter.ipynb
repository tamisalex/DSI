{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Metrics and Loss Functions\n",
    "\n",
    "We've seen two examples of _loss functions_ earlier in the week in the context of regularization:\n",
    "* The sum of squared errors:\n",
    "$$\\sum_{i}{\\left(\\hat{y}_i - y_i \\right)^2}$$\n",
    "* Regularlized versions\n",
    "$$\\sum_{i}{\\left(\\hat{y}_i - y_i \\right)^2 + \\alpha \\theta_i}$$\n",
    "\n",
    "In this lesson we're going to dig deeper into loss functions and their applications. Different loss functions are useful in different scenarios and there are two very popular loss functions that are used in conjuction with regression. In this case they are sometimes referred to as _regression metrics_.\n",
    "\n",
    "The first is the _root mean squared error_ or _RMSE_ and it is the mean of the squared errors. If we have $n$ regression points and their predictions, the [RMSE](https://en.wikipedia.org/wiki/Root-mean-square_deviation) is:\n",
    "\n",
    "$$\\text{RMSE} = \\sqrt{\\frac{\\sum_{i}{\\left(\\hat{y}_i - y_i \\right)^2}}{n}}$$\n",
    "\n",
    "The second is the _mean absolute error_ or _MAE_, and it differs by use of an absolute value instead of a square. The [MAE](https://en.wikipedia.org/wiki/Average_absolute_deviation) is:\n",
    "\n",
    "$$\\text{MAE} = \\frac{\\sum_{i}{|\\hat{y}_i - y_i |}}{n}$$\n",
    "\n",
    "## Why have different regression metrics?\n",
    "\n",
    "You might be thinking, _what's all the fuss about_? It turns out that there are lots of good reasons to use different loss functions. We've seen one -- regularization -- and now we'll consider the effects of outliers on these two metrics.\n",
    "\n",
    "First let's try a very simplified statistics problem. Given a dataset, how can we summarize it with a single number? Do you know any ways?\n",
    "\n",
    "This is equivalent to fitting a constant model to the data. It turns out that the _mean_ minimizes the RMSE and the _median_ minimizes the MAE. By analogy, when fitting a model, MAE is more tolerant to outliers. In other words, the degree of error of an outlier has a large impact when using RMSE versus the MAE. Since the choice of loss function affects model fit, it's important to consider how you want errors to impact your models.\n",
    "\n",
    "**Summary**\n",
    "* Use MAE when how far off an error is makes little difference\n",
    "* Use RMSE when more extreme errors should have a large impact\n",
    "\n",
    "Finally, note that linear regressions with MAE instead of RMSE are called _least absolute deviation_ regressions rather than least squares regressions.\n",
    "\n",
    "### Bonus: Modes\n",
    "\n",
    "It turns out the _mode_ minimizes the sum:\n",
    "$$\\frac{\\sum_{i}{|\\hat{y}_i - y_i |^{0}}}{n}$$\n",
    "where $0^0=0$ and $x^0=1$ otherwise. Can you see why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guided practice\n",
    "\n",
    "Let's compute the RMSE and the MAE for a sample data set. Let's say we had a quadratic function that we fit a line to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1, 2, 5, 10]\n",
      "[-2, 0, 2, 4, 6]\n"
     ]
    }
   ],
   "source": [
    "xs = [-1, 0, 1, 2, 3]\n",
    "ys = [x*x + 1 for x in xs] # true values\n",
    "predictions = [2*x for x in xs]\n",
    "print ys\n",
    "print predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First do the calculation by hand to see how large each term is\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 2.60768096208\n",
      "MAE: 2.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "print \"RMSE:\", mean_squared_error(ys, predictions)**.5\n",
    "print \"MAE:\", mean_absolute_error(ys, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add an outlier to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 33.8333333333\n",
      "MAE: 3.83333333333\n"
     ]
    }
   ],
   "source": [
    "xs.append(4)\n",
    "ys.append(17)\n",
    "predictions.append(30)\n",
    "\n",
    "print \"RMSE:\", mean_squared_error(ys, predictions)\n",
    "print \"MAE:\", mean_absolute_error(ys, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the impact on the RMSE was large, a factor of 8, versus the impact on the MAE with a factor of 2.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indepedent Practice\n",
    "\n",
    "Let's explore two scenarios to obtain a better understanding of RMSE and MAE. First let's fit two models to the same set of data, the data above. To do the least mean absolute error we will use `statsmodels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "# Make the plots bigger\n",
    "plt.rcParams['figure.figsize'] = 10, 10\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   x   y\n",
      "0 -1   2\n",
      "1  0   1\n",
      "2  1   2\n",
      "3  2   5\n",
      "4  3  10\n",
      "5  4  17\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "For numerical factors, num_columns must be an int",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-ffde9e7a5910>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"x\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"y\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msmf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquantreg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'y ~ x'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Alex Tam\\Anaconda2\\lib\\site-packages\\statsmodels\\base\\model.pyc\u001b[0m in \u001b[0;36mfrom_formula\u001b[1;34m(cls, formula, data, subset, *args, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m         (endog, exog), missing_idx = handle_formula_data(data, None, formula,\n\u001b[0;32m    146\u001b[0m                                                          \u001b[0mdepth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meval_env\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m                                                          missing=missing)\n\u001b[0m\u001b[0;32m    148\u001b[0m         kwargs.update({'missing_idx': missing_idx,\n\u001b[0;32m    149\u001b[0m                        'missing': missing})\n",
      "\u001b[1;32mC:\\Users\\Alex Tam\\Anaconda2\\lib\\site-packages\\statsmodels\\formula\\formulatools.pyc\u001b[0m in \u001b[0;36mhandle_formula_data\u001b[1;34m(Y, X, formula, depth, missing)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdata_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_using_pandas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m             result = dmatrices(formula, Y, depth, return_type='dataframe',\n\u001b[1;32m---> 65\u001b[1;33m                                NA_action=na_action)\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             result = dmatrices(formula, Y, depth, return_type='dataframe',\n",
      "\u001b[1;32mC:\\Users\\Alex Tam\\Anaconda2\\lib\\site-packages\\patsy\\highlevel.pyc\u001b[0m in \u001b[0;36mdmatrices\u001b[1;34m(formula_like, data, eval_env, NA_action, return_type)\u001b[0m\n\u001b[0;32m    295\u001b[0m     \u001b[0meval_env\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEvalEnvironment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcapture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meval_env\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreference\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m     (lhs, rhs) = _do_highlevel_design(formula_like, data, eval_env,\n\u001b[1;32m--> 297\u001b[1;33m                                       NA_action, return_type)\n\u001b[0m\u001b[0;32m    298\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlhs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mPatsyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"model is missing required outcome variables\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Alex Tam\\Anaconda2\\lib\\site-packages\\patsy\\highlevel.pyc\u001b[0m in \u001b[0;36m_do_highlevel_design\u001b[1;34m(formula_like, data, eval_env, NA_action, return_type)\u001b[0m\n\u001b[0;32m    150\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m     design_infos = _try_incr_builders(formula_like, data_iter_maker, eval_env,\n\u001b[1;32m--> 152\u001b[1;33m                                       NA_action)\n\u001b[0m\u001b[0;32m    153\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdesign_infos\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m         return build_design_matrices(design_infos, data,\n",
      "\u001b[1;32mC:\\Users\\Alex Tam\\Anaconda2\\lib\\site-packages\\patsy\\highlevel.pyc\u001b[0m in \u001b[0;36m_try_incr_builders\u001b[1;34m(formula_like, data_iter_maker, eval_env, NA_action)\u001b[0m\n\u001b[0;32m     55\u001b[0m                                       \u001b[0mdata_iter_maker\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m                                       \u001b[0meval_env\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m                                       NA_action)\n\u001b[0m\u001b[0;32m     58\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Alex Tam\\Anaconda2\\lib\\site-packages\\patsy\\build.pyc\u001b[0m in \u001b[0;36mdesign_matrix_builders\u001b[1;34m(termlists, data_iter_maker, eval_env, NA_action)\u001b[0m\n\u001b[0;32m    704\u001b[0m                             \u001b[0mfactor_states\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfactor\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    705\u001b[0m                             \u001b[0mnum_columns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_column_counts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfactor\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 706\u001b[1;33m                             categories=None)\n\u001b[0m\u001b[0;32m    707\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    708\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0mfactor\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcat_levels_contrasts\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Alex Tam\\Anaconda2\\lib\\site-packages\\patsy\\design_info.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, factor, type, state, num_columns, categories)\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"numerical\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_columns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                 raise ValueError(\"For numerical factors, num_columns \"\n\u001b[0m\u001b[0;32m     89\u001b[0m                                  \"must be an int\")\n\u001b[0;32m     90\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcategories\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: For numerical factors, num_columns must be an int"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(np.array([xs, ys]).transpose(), columns=[\"x\", \"y\"])\n",
    "df.columns = [\"x\", \"y\"]\n",
    "print df\n",
    "mod = smf.quantreg('y ~ x', df)\n",
    "res = mod.fit(q=.5)\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This generated a fit of $y = 4 x + 1$. Let's see what a linear regression yields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "X = np.array(xs).transpose()\n",
    "X = sm.add_constant(X)\n",
    "# Fit and summarize OLS model\n",
    "mod = sm.OLS(ys, X)\n",
    "res = mod.fit()\n",
    "print res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This yielded a fit of $y = 3.125 x + 1.5625$.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Plot the data with both functions. Which do you think fits the data better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f1 = lambda x: 4*x + 1\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's explore another scenario. Linear regression has five major assumptions, one of which is called _constant variance_ or _homoscedasticity_. It means that the errors are distributed with the same variance about the best fit line regardless of the value of the independent variables.\n",
    "\n",
    "In practice this means that data with a persistant level of background noise can cause regression metrics to be poorly estimated. Let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from scipy.stats import norm\n",
    "# Generate some data\n",
    "xs = list(np.arange(0, 10, 0.1))\n",
    "ys = [2*x + norm.pdf(0, 1) for x in xs]\n",
    "# Add random background noise\n",
    "xs2 = [10 * random.random() for i in range(20)]\n",
    "ys2 = [20 * random.random() for i in range(20)]\n",
    "\n",
    "# Plot the data sets\n",
    "plt.scatter(xs, ys, color='b')\n",
    "plt.scatter(xs2, ys2, color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Combine the data\n",
    "xs.extend(xs2)\n",
    "ys.extend(ys2)\n",
    "df = pd.DataFrame(np.array([xs, ys]).transpose(), columns=['x', 'y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fit a line to the data\n",
    "# Compute the RMSE and the MAE\n",
    "# Plot the regression line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now try a MAE regression with statsmodels and plot it.\n",
    "# You should see a much better fit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the data and the two fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
